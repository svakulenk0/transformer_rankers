{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !mkdir data\n",
    "# # !mkdir data/clariq\n",
    "# !cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/dev.tsv\n",
    "# !cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/train.tsv\n",
    "# !cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/question_bank.tsv\n",
    "# !mv data/clariq/train.tsv data/clariq/train_original.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path = \"./data/\"\n",
    "\n",
    "train = pd.read_csv(data_path+\"clariq/train_original.tsv\", sep=\"\\t\")\n",
    "valid = pd.read_csv(data_path+\"clariq/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "train = train[[\"initial_request\", \"question\"]]\n",
    "train.columns = [\"query\", \"clarifying_question\"]\n",
    "train = train[~train[\"clarifying_question\"].isnull()]\n",
    "\n",
    "valid = valid[[\"initial_request\", \"question\"]]\n",
    "valid.columns = [\"query\", \"clarifying_question\"]\n",
    "valid = valid[~valid[\"clarifying_question\"].isnull()]\n",
    "\n",
    "train.to_csv(data_path+\"clariq/train.tsv\", sep=\"\\t\", index=False)\n",
    "valid.to_csv(data_path+\"clariq/valid.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>clarifying_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me about Obama family tree.</td>\n",
       "      <td>are you interested in seeing barack obamas family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tell me about Obama family tree.</td>\n",
       "      <td>would you like to know barack obamas geneology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me about Obama family tree.</td>\n",
       "      <td>would you like to know about obamas ancestors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tell me about Obama family tree.</td>\n",
       "      <td>would you like to know who is currently alive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tell me about Obama family tree.</td>\n",
       "      <td>are you looking for biological information on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              query  \\\n",
       "0  Tell me about Obama family tree.   \n",
       "1  Tell me about Obama family tree.   \n",
       "2  Tell me about Obama family tree.   \n",
       "3  Tell me about Obama family tree.   \n",
       "4  Tell me about Obama family tree.   \n",
       "\n",
       "                                 clarifying_question  \n",
       "0  are you interested in seeing barack obamas family  \n",
       "1     would you like to know barack obamas geneology  \n",
       "2      would you like to know about obamas ancestors  \n",
       "3  would you like to know who is currently alive ...  \n",
       "4  are you looking for biological information on ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For transformer-rankers we only need a pandas DF with query (here the initial request) \n",
    "# and relevant documents (here the clarifying questions).\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q00001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q00002</td>\n",
       "      <td>a total cholesterol of 180 to 200 mgdl 10 to 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q00003</td>\n",
       "      <td>about how many years experience do you want th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q00004</td>\n",
       "      <td>according to anima the bible or what other source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q00005</td>\n",
       "      <td>ae you looking for examples of septic system d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id                                           question\n",
       "0      Q00001                                                NaN\n",
       "1      Q00002  a total cholesterol of 180 to 200 mgdl 10 to 1...\n",
       "2      Q00003  about how many years experience do you want th...\n",
       "3      Q00004  according to anima the bible or what other source\n",
       "4      Q00005  ae you looking for examples of septic system d..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will sample negative samples for training using the question bank\n",
    "question_bank = pd.read_csv(data_path+\"clariq/question_bank.tsv\", sep=\"\\t\")\n",
    "question_bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/vendi/transformer_rankers-notebooks\" target=\"_blank\">https://app.wandb.ai/vendi/transformer_rankers-notebooks</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/vendi/transformer_rankers-notebooks/runs/10yob1su\" target=\"_blank\">https://app.wandb.ai/vendi/transformer_rankers-notebooks/runs/10yob1su</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:13:59,616 [ERROR] Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:13:59,738 [INFO] system metrics and metadata threads started\n",
      "2020-12-15 12:13:59,741 [INFO] checking resume status, waiting at most 10 seconds\n",
      "2020-12-15 12:13:59,903 [INFO] resuming run from id: UnVuOnYxOjEweW9iMXN1OnRyYW5zZm9ybWVyX3JhbmtlcnMtbm90ZWJvb2tzOnZlbmRp\n",
      "2020-12-15 12:13:59,916 [INFO] upserting run before process can begin, waiting at most 10 seconds\n",
      "2020-12-15 12:14:00,103 [INFO] saving pip packages\n",
      "2020-12-15 12:14:00,110 [INFO] initializing streaming files api\n",
      "2020-12-15 12:14:00,111 [INFO] unblocking file change observer, beginning sync with W&B servers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/vendi/transformer_rankers-notebooks/runs/10yob1su"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:00,120 [INFO] shutting down system stats and metadata service\n",
      "2020-12-15 12:14:00,250 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/config.yaml\n",
      "2020-12-15 12:14:00,427 [INFO] file/dir created: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n",
      "2020-12-15 12:14:00,437 [INFO] file/dir created: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n",
      "2020-12-15 12:14:00,439 [INFO] file/dir created: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-summary.json\n",
      "2020-12-15 12:14:00,443 [INFO] file/dir created: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-history.jsonl\n",
      "2020-12-15 12:14:00,450 [INFO] file/dir created: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/requirements.txt\n",
      "2020-12-15 12:14:00,743 [INFO] stopping streaming files and file change observer\n",
      "2020-12-15 12:14:01,250 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n",
      "2020-12-15 12:14:01,255 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:03,527 [INFO] Train instances per batch 48\n",
      "2020-12-15 12:14:03,562 [INFO] Loading instances from ./data/clariq//pointwise_set_train_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n",
      "2020-12-15 12:14:03,894 [INFO] Total of 16981 instances were cached.\n",
      "2020-12-15 12:14:03,906 [INFO] Loading instances from ./data/clariq//pointwise_set_val_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n",
      "2020-12-15 12:14:04,060 [INFO] Total of 2025 instances were cached.\n",
      "2020-12-15 12:14:04,072 [INFO] Loading instances from ./data/clariq//pointwise_set_test_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n",
      "2020-12-15 12:14:04,086 [INFO] Total of 2025 instances were cached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:08,017 [INFO] Device cuda\n",
      "2020-12-15 12:14:08,017 [INFO] Num GPU 4\n",
      "2020-12-15 12:14:08,095 [INFO] Total batches per epoch : 354\n",
      "2020-12-15 12:14:08,095 [INFO] Validating every 1 epoch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0, steps:   0%|          | 0/354 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:12,545 [ERROR] Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svakule/anaconda3/envs/ranking/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:12,679 [INFO] system metrics and metadata threads started\n",
      "2020-12-15 12:14:12,680 [INFO] checking resume status, waiting at most 10 seconds\n",
      "2020-12-15 12:14:12,829 [INFO] resuming run from id: UnVuOnYxOjEweW9iMXN1OnRyYW5zZm9ybWVyX3JhbmtlcnMtbm90ZWJvb2tzOnZlbmRp\n",
      "2020-12-15 12:14:12,840 [INFO] upserting run before process can begin, waiting at most 10 seconds\n",
      "2020-12-15 12:14:13,012 [INFO] saving pip packages\n",
      "2020-12-15 12:14:13,017 [INFO] initializing streaming files api\n",
      "2020-12-15 12:14:13,019 [INFO] unblocking file change observer, beginning sync with W&B servers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:   0%|          | 1/354 [00:05<30:32,  5.19s/it]/home/svakule/anaconda3/envs/ranking/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:13,546 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-summary.json\n",
      "2020-12-15 12:14:13,550 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/requirements.txt\n",
      "2020-12-15 12:14:13,552 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/config.yaml\n",
      "2020-12-15 12:14:13,732 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n",
      "2020-12-15 12:14:13,736 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  11%|█         | 39/354 [00:21<02:11,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:29,571 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  21%|██        | 73/354 [00:35<01:54,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:43,597 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  22%|██▏       | 78/354 [00:37<01:54,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:14:45,600 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  33%|███▎      | 117/354 [00:53<01:37,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:15:01,639 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  42%|████▏     | 150/354 [01:07<01:29,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:15:15,780 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  44%|████▍     | 155/354 [01:09<01:21,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:15:17,783 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  55%|█████▍    | 194/354 [01:25<01:05,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:15:33,810 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  64%|██████▍   | 228/354 [01:39<00:51,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:15:47,833 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  66%|██████▌   | 233/354 [01:41<00:50,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:15:49,836 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  77%|███████▋  | 271/354 [01:57<00:34,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:16:05,927 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  86%|████████▌ | 305/354 [02:11<00:20,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:16:20,046 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  88%|████████▊ | 310/354 [02:13<00:19,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:16:22,049 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps:  99%|█████████▊| 349/354 [02:29<00:02,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:16:38,147 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, steps: 100%|██████████| 354/354 [02:31<00:00,  2.33it/s]\n",
      "Predicting:  59%|█████▊    | 99/169 [00:12<00:08,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:16:52,195 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  69%|██████▊   | 116/169 [00:14<00:06,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:16:54,203 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 169/169 [00:20<00:00,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:17:00,693 [INFO] shutting down system stats and metadata service\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 12:17:01,223 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-summary.json\n",
      "2020-12-15 12:17:01,227 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-history.jsonl\n",
      "2020-12-15 12:17:01,230 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-events.jsonl\n",
      "2020-12-15 12:17:01,241 [INFO] stopping streaming files and file change observer\n",
      "2020-12-15 12:17:02,226 [INFO] file/dir modified: /home/svakule/transformer_rankers/notebooks/wandb/run-20201215_121358-10yob1su/wandb-metadata.json\n"
     ]
    }
   ],
   "source": [
    "from transformer_rankers.trainers import transformer_trainer\n",
    "from transformer_rankers.datasets import dataset\n",
    "from transformer_rankers.negative_samplers import negative_sampling\n",
    "from transformer_rankers.eval import results_analyses_tools\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "logging.basicConfig(\n",
    "  level=logging.INFO,\n",
    "  format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "  handlers=[\n",
    "      logging.StreamHandler(sys.stdout)\n",
    "  ]\n",
    ")\n",
    "\n",
    "#The combination of query and question are not that big.\n",
    "max_seq_len = 50\n",
    "\n",
    "#Lets use an almost balanced amount of positive and negative samples during training.\n",
    "average_relevant_per_query = train.groupby(\"query\").count().mean().values[0]\n",
    "\n",
    "#Instantiate BM25 negative sampler.\n",
    "ns_train = negative_sampling.BM25NegativeSamplerPyserini(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query) , \n",
    "                    \"./data/clariq/anserini_train/\", -1, \"./anserini/\")\n",
    "ns_val = negative_sampling.BM25NegativeSamplerPyserini(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query), \n",
    "                    \"./data/clariq/anserini_train/\", -1, \"./anserini/\")\n",
    "\n",
    "# We could also use random sampling which does not require Anserini.\n",
    "# ns_train = negative_sampling.RandomNegativeSampler(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query))\n",
    "# ns_val = negative_sampling.RandomNegativeSampler(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query))\n",
    "\n",
    "#Create the loaders for the dataset, with the respective negative samplers\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataloader = dataset.QueryDocumentDataLoader(train_df=train,\n",
    "                    val_df=valid, test_df=valid,\n",
    "                    tokenizer=tokenizer, negative_sampler_train=ns_train,\n",
    "                    negative_sampler_val=ns_val, task_type='classification',\n",
    "                    train_batch_size=12, val_batch_size=12, max_seq_len=max_seq_len,\n",
    "                    sample_data=-1, cache_path=\"./data/clariq/\")\n",
    "\n",
    "train_loader, val_loader, test_loader = dataloader.\\\n",
    "  get_pytorch_dataloaders()\n",
    "\n",
    "#Use BERT (any model that has SequenceClassification class from HuggingFace would work here)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#Instantiate trainer that handles fitting.\n",
    "trainer = transformer_trainer.TransformerTrainer(model=model,\n",
    "  train_loader=train_loader,\n",
    "  val_loader=val_loader, test_loader=test_loader,\n",
    "  num_ns_eval=int(average_relevant_per_query), task_type=\"classification\", tokenizer=tokenizer,\n",
    "  validate_every_epochs=1, num_validation_batches=-1,\n",
    "  num_epochs=1, lr=5e-7, sacred_ex=None)\n",
    "\n",
    "#Train (our validation eval uses the NS sampling procedure)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/svakule/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/svakule/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "rerank_top_k = 30\n",
    "# Imports required packages, defines stem & tokenizez function\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stem_tokenize(text, remove_stopwords=True):\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens = [word for sent in nltk.sent_tokenize(text) \\\n",
    "                                      for word in nltk.word_tokenize(sent)]\n",
    "  tokens = [word for word in tokens if word not in \\\n",
    "          nltk.corpus.stopwords.words('english')]\n",
    "  return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Files paths\n",
    "request_file_path = './ClariQ-repo/data/dev.tsv'\n",
    "question_bank_path = './ClariQ-repo/data/question_bank.tsv'\n",
    "run_file_path = './ClariQ-repo/sample_runs/dev_bm25'\n",
    "\n",
    "# Reads files and build bm25 corpus (index)\n",
    "dev = pd.read_csv(request_file_path, sep='\\t')\n",
    "question_bank = pd.read_csv(question_bank_path, sep='\\t').fillna('')\n",
    "question_bank['tokenized_question_list'] = question_bank['question'].map(stem_tokenize)\n",
    "question_bank['tokenized_question_str'] = question_bank['tokenized_question_list'].map(lambda x: ' '.join(x))\n",
    "bm25_corpus = question_bank['tokenized_question_list'].tolist()\n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "\n",
    "# Runs bm25 for every query and stores output in file.\n",
    "examples = []\n",
    "all_preds_bm25 = []\n",
    "with open(run_file_path, 'w') as fo:\n",
    "  for tid in dev['topic_id'].unique():\n",
    "    query = dev.loc[dev['topic_id']==tid, 'initial_request'].tolist()[0]\n",
    "    bm25_ranked_list = bm25.get_top_n(stem_tokenize(query, True), \n",
    "                                    bm25_corpus, \n",
    "                                    n=rerank_top_k)\n",
    "    bm25_q_list = [' '.join(sent) for sent in bm25_ranked_list]\n",
    "    docs = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question'].tolist()\n",
    "    preds = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question_id'].tolist()\n",
    "    all_preds_bm25.append(preds)\n",
    "    for doc in docs[:rerank_top_k]:\n",
    "      examples.append((query, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/svakule/anaconda3/envs/ranking/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers.data.data_collator import default_data_collator\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformer_rankers.utils import utils\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index]\n",
    "\n",
    "batch_encoding = tokenizer.batch_encode_plus(examples, \n",
    "                max_length=max_seq_len, pad_to_max_length=True)\n",
    "features = []\n",
    "for i in range(len(examples)):\n",
    "    inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "    feature = InputFeatures(**inputs, label=0)\n",
    "    features.append(feature)\n",
    "\n",
    "dataset = SimpleDataset(features)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 94/94 [00:11<00:00,  7.97it/s]\n"
     ]
    }
   ],
   "source": [
    "logits, _, softmax_output = trainer.predict(dataloader)\n",
    "softmax_output_by_query = utils.acumulate_list(softmax_output[0], rerank_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "run_file_path = './ClariQ-repo/sample_runs/dev_BERT-reranker'\n",
    "with open(run_file_path, 'w') as fo:\n",
    "  for tid_idx, tid in enumerate(dev['topic_id'].unique()):\n",
    "    document_scores = np.array(softmax_output_by_query[tid_idx])\n",
    "    top_k_scores_idx = (-document_scores).argsort()[:rerank_top_k]  \n",
    "    preds = np.array(all_preds_bm25[tid_idx])[top_k_scores_idx]\n",
    "    for i, qid in enumerate(preds):\n",
    "      fo.write('{} 0 {} {} {} BERT-reranker\\n'.format(tid, qid, i, len(preds)-i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall5: 0.10574913811678517\r\n",
      "Recall10: 0.22855252384354552\r\n",
      "Recall20: 0.38955943024983275\r\n",
      "Recall30: 0.6912818698329535\r\n"
     ]
    }
   ],
   "source": [
    "# Report question relevance performance\n",
    "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task question_relevance\\\n",
    "                                                --data_dir ./ClariQ-repo/data/ \\\n",
    "                                                --experiment_type dev \\\n",
    "                                                --run_file {run_file_path} \\\n",
    "                                                --out_file {run_file_path}_question_relevance.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./ClariQ-repo/src/clariq_eval_tool.py\", line 274, in <module>\r\n",
      "    main()\r\n",
      "  File \"./ClariQ-repo/src/clariq_eval_tool.py\", line 267, in main\r\n",
      "    input_args.out_file, input_args.multi_turn, input_args.leaderboard)\r\n",
      "  File \"./ClariQ-repo/src/clariq_eval_tool.py\", line 101, in evaluate_document_relevance\r\n",
      "    return evaluate_document_relevance_single_turn(experiment_type, data_dir, run_file, out_file, leaderboard)\r\n",
      "  File \"./ClariQ-repo/src/clariq_eval_tool.py\", line 45, in evaluate_document_relevance_single_turn\r\n",
      "    eval_dict = load_eval_dict(eval_file_path, topic_file_path)\r\n",
      "TypeError: load_eval_dict() missing 1 required positional argument: 'multi_turn'\r\n"
     ]
    }
   ],
   "source": [
    "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task document_relevance\\\n",
    "                                                --data_dir ./ClariQ-repo/data/ \\\n",
    "                                                --experiment_type dev \\\n",
    "                                                --run_file {run_file_path} \\\n",
    "                                                --out_file {run_file_path}.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranking",
   "language": "python",
   "name": "ranking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
